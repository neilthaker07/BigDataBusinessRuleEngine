package big.data

import org.apache.spark.SparkConf
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
import org.apache.spark.SparkContext

object t1 {
  def main(args : Array[String])
  {
    print("998888888888888")

    val conf = new SparkConf().setAppName("SampleApp").setMaster("local[2]")//.set("spark.ui.port","7077")
   /* val sparkSession = SparkSession.builder().master("local").config("","").getOrCreate()
    
    val df1 = sparkSession.read.json("/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/git_ws_scala_spark_drools/BigDataBusinessRuleEngine/finaldemo/logs/log1.json")
    df1.select("TEMPERATURE").show()
    
    val tList = df1.select("TEMPERATURE").collectAsList()
    val t = tList.get(2).toString()
    val t2 = t.substring(1, t.length()-1)
    val temperatureLogFiles = t2.toDouble

    println("******************************************")
    println(temperatureLogFiles)
    
    val n = 150
    val nodeRedTemperature = n.toDouble
    
    if(temperatureLogFiles > nodeRedTemperature)
    {
      println("--------------+++++++++++++++++---------------+++++++++++")
      println(temperatureLogFiles)
    }
    */
    
    val sc = new SparkContext(conf)
    val ssc = new StreamingContext(sc, Seconds(5))
    val input = ssc.textFileStream("file:///home/neil/Neil_Work/MS_SJSU/scala_spark_learning/Spark_Scala/git_ws_scala_spark_drools/BigDataBusinessRuleEngine/finaldemo/logs/"+"///")
    print("+++++++++++++++++++++++++")
    input.print()
    
    
    val sqlContext= new org.apache.spark.sql.SQLContext(sc)
    import sqlContext.implicits._
    input.foreachRDD { rdd =>
      var df = rdd.toDF()
      print("**********************1111111111111****************")
      df.show()
    }
    
    
    print("/////////////////////////"+input.toString())
    ssc.start()
    ssc.awaitTermination()
    
    
    
    
    
    
    
    
    
  }
}